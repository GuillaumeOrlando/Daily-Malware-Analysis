# Gorgon Group APT - 
# Stage 0: Blog script extractor
# Stage 1: Auto-deobfuscation of the embeeded JS scripts

import re
import os
import hashlib
import datetime
import requests
from node_vm2 import eval

def read_target(path):
    re = []
    with open(path, 'r') as f:
        content = f.read()
    for line in content.split('\n'):
        if line != '' and line[:1] != '#':
            #re.append('http://' + line)
            re.append(line)
    return re


def grab_html(url):
    return requests.get(url).text


def grab_scripts(content):
    result = []
    a = re.split('<script>\n<!--', content)
    for elem in a:
        if '-->\n</script>' in elem:
            result.append(elem.split('</script>')[0])
    return result


def eval_script(scripts, name):
    name = name.replace('http://', '').replace('.', '_').replace('/', '_')
    new = ''
    for script in scripts:
        for line in script.replace('//-->', '').split('\n'):
            if line != '\n':
                new += line
        if 'document.write(' in new:
            js_target = new.replace('document.write(', '').replace('));', ');')
            a = eval(js_target)
            pa = './Export/%s/%s.dmp' % (datetime.date.today(), name)
            with open(pa, 'a+') as f:
                f.write('\n#Auto generated\n#' + str(datetime.date.today()) + '\n\n' + a)
            print('[+] Saved under ./Export/%s/%s' % (datetime.date.today(), name))
        else:
            print('[-] Unable to process this script. Update me.')
            return 'None'
        
if not os.path.isdir('./Export/%s' % datetime.date.today()):
    os.mkdir('./Export/%s' % datetime.date.today())

# Extract the VBA layers
config = './Config/blog.txt'
targets = read_target(config)
for target in targets:
    print('[*] Processing %s' % target)
    html = grab_html(target)
    if not 'Le blog à été supprimé' in html:
        script = grab_scripts(html)
        eval_script(script, target)
        if script == []:
            print('[-] Site is inactive')
    print('')

url = []
d = './Export/' + str(datetime.date.today())
for fa in os.listdir(d):
    with open('%s/%s' % (d, fa)) as f:
        content = f.read()#.replace('"', '').replace("'", '')
    for line in content.split('\n'):
        if 'https' in line:
            v = 'https' + line.split('https')[1].split('"')[0].split("'")[0]
            if v not in url:
                url.append(v)
        elif 'http:' in line and 'https' not in line:
            v = 'http' + line.split('http')[1].split('"')[0].split("'")[0]
            if v not in url:
                url.append(v)

blogpost = []
bitbucket = []
payload = []


for elem in url:
    if 'blogspot' in elem:
        blogpost.append(elem)
    elif 'bitbucket' in elem:
        bitbucket.append(elem)
    else:
        payload.append(elem)

print('Blogpost: ')
print(blogpost)

print('Bitbucket: ')
print(bitbucket)

print('Payloads: ')
print(payload)

for elem in payload:
    a = requests.get(elem)
    c = a.text
    hash_object = hashlib.md5(c.encode())
    md5_hash = hash_object.hexdigest()
    with open('./Payloads/%s.dmp' % (str(md5_hash)), 'w+') as f:
        f.write(c)
    print('Payload extract: %s' % ('./Payloads/' + md5_hash +'.dmp'))
